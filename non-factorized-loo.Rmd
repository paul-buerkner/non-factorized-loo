---
title: "Leave-one-out cross-validation for non-factorized models"
author: Aki Vehtari & Paul Buerkner
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache=TRUE, message=FALSE, error=FALSE,
  warning=FALSE, comment=NA, out.width='95%'
)
```

## Introduction

When computing approximate leave-one-out cross-validation (LOO-CV) after
fitting a Bayesian model, the first step is to calculate the *pointwise*
log-likelihood for every response value $y_i, \: i = 1, \ldots, N$.
This is straightforward for *factorized* models in which response values are
conditionally independent given the model parameters $\theta$ and the likelihood
can be written in the familiar form
$$
p(y \,|\, \theta) = \prod_{i=1}^N p(y_i \,|\, \theta).
$$
The function $p$ will be either a probability density function or a probability
mass function depending on whether we have a continuous or discrete outcome.
When $p(y)$ can be factorized in this way, the conditional pointwise
log-likelihood can be obtained easily by computing $\log p(y_i \,|\, \theta)$ for
each $i$. We then save each of these individual contributions to the
log-likelihood rather than simply summing them to obtain the total
log-likelihood.

Unfortunately, the situation is more complicated for *non-factorized* models in
which response values are not conditionally independent. When there is residual
dependency even after accounting for the model parameters $\theta$, the
conditional pointwise log-likelihood has the general form
$\log p(y_i \,|\, y_{-i}, \theta)$, where $y_{-i}$ denotes all response values
except observation $i$.

## LOO-CV for multivariate normal models

Although computing the pointwise log-likelihood for non-factorized models is
often impossible, there is a large class of multivariate normal models
for which an analytical solution is available. These equations were initially
derived by Sundararajan and Keerthi (2001) with a focus on the special case of a
zero-mean Gaussian process model with prior covariance $K$ and residual
standard deviation $\sigma$,
$$
y \sim {\mathrm N}(0, \, K+\sigma^2 I),
$$
where $I$ is the identity matrix of appropriate dimension.

Fortunately, Sundararajan and Keerthi's results also generalize to the case of
an arbitrary nondegenerate invertible covariance matrix $C$. For such models,
the LOO predictive mean and standard deviation can be computed as follows:
\begin{align}
  \mu_{\tilde{y},-i} &= y_i-\bar{c}_{ii}^{-1} g_i \nonumber \\
  \sigma_{\tilde{y},-i} &= \sqrt{\bar{c}_{ii}^{-1}},
\end{align}
where
\begin{align}
  g_i &= \left[C^{-1} y\right]_i \nonumber \\
  \bar{c}_{ii} &= \left[C^{-1}\right]_{ii}.
\end{align}

Using these results, the log predictive density of the $i$th observation is then
computed as
$$
  \log p(y_i \,|\, y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  - \frac{1}{2}\log \sigma^2_{-i}
  - \frac{1}{2}\frac{(y_i-\mu_{-i})^2}{\sigma^2_{-i}}.
$$
Expressing this same equation in terms of $g_i$ and $\bar{c}_{ii}$, the log
predictive density becomes:
$$
  \log p(y_i \,|\, y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  + \frac{1}{2}\log \bar{c}_{ii}
  - \frac{1}{2}\frac{g_i^2}{\bar{c}_{ii}}.
$$
(Note that Vehtari et al. (2016) has a typo in the corresponding Equation 34.)

From these equations we can now derive a recipe for obtaining the conditional
pointwise log-likelihood for _all_ models that can be expressed conditionally in
terms of a multivariate normal with invertible covariance matrix $C$.

### Approximate LOO-CV using integrated importance-sampling

The above LOO equations for multivariate normal models are conditional on
parameters $\theta$. Therefore, to obtain the leave-one-out predictive density
$p(y_i \,|\, y_{-i})$ we need to integrate over $\theta$,
$$
p(y_i\,|\,y_{-i}) =
  \int p(y_i\,|\,y_{-i}, \theta) \, p(\theta\,|\,y_{-i}) \,d\theta.
$$
Here, $p(\theta\,|\,y_{-i})$ is the leave-one-out posterior distribution for
$\theta$, that is, the posterior distribution for $\theta$ obtained by fitting
the model while holding out the $i$th observation. To avoid the cost of
sampling from $N$ leave-one-out posteriors, it is possible to take the posterior
draws $\theta^{(s)}, \, s=1,\ldots,S$, from the \emph{full} posterior
$p(\theta\,|\,y)$, and then approximate the above integral using integrated
importance sampling (Vehtari et al., 2016, Section 3.6.1):
$$
 p(y_i\,|\,y_{-i}) \approx
   \frac{ \sum_{s=1}^S p(y_i\,|\,y_{-i},\,\theta^{(s)}) \,w_i^{(s)}}{ \sum_{s=1}^S w_i^{(s)}},
$$
where $w_i^{(s)}$ are importance weights.

First we compute the raw importance ratios
$$
  r_i^{(s)} \propto \frac{1}{p(y_i \,|\, y_{-i}, \,\theta^{(s)})},
$$
and then stabilize them using Pareto smoothed importance sampling (PSIS, Vehtari
et al, 2017b) to obtain the weights $w_i^{(s)}$. The resulting approximation is
referred to as PSIS-LOO (Vehtari et al, 2017a).

### Exact LOO-CV with re-fit

In order to validate the approximate LOO procedure, and also in order to allow
exact computations to be made for a small number of leave-one-out folds for which
the Pareto $k$ diagnostic (Vehtari et al, 2017b) indicates an unstable
approximation, we need to consider how we might to do _exact_ leave-one-out CV
for a non-factorizable model. In the case of a Gaussian process that has the
marginalization property, we could just drop the one row and column of $C$
corresponding to the held out out observation. This does not hold in general,
however, and to keep the original prior we need to maintain the full covariance
matrix $C$ even when one of the observations is left out.

The solution is to model $y_i$ as a missing observation and estimate it along
with all of the other model parameters. For a conditional multivariate normal
model, $\log p(y_i\,|\,y_{-i})$ can be computed as follows. First, we model
$y_i$ as missing and denote the corresponding parameter $y_{\mathrm{mis}}$.
Then, we define
$$
y_{\mathrm{mis}(i)} = (y_1, \ldots, y_{i-1}, y_{\mathrm{mis}}, y_{i+1}, \ldots, y_N).
$$
to be the same as the full set of observations $y$, except replacing $y_i$
with the parameter $y_{\mathrm{mis}}$.

Second, we compute the LOO predictive mean and standard deviations as above, but
replace $y$ with $y_{\mathrm{mis}(i)}$ in the computation of
$\mu_{\tilde{y},-i}$:
$$
\mu_{\tilde{y},-i} = y_{{\mathrm{mis}}(i)}-\bar{c}_{ii}^{-1}g_i,
$$
where
$$
g_i = \left[ C^{-1} y_{\mathrm{mis}(i)} \right]_i.
$$

The conditional log predictive density is then computed with the above
$\mu_{\tilde{y},-i}$ and the left out observation $y_i$

$$
  \log p(y_i\,|\,y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  - \frac{1}{2}\log \sigma^2_{\tilde{y},-i}
  - \frac{1}{2}\frac{(y_i-\mu_{\tilde{y},-i})^2}{\sigma^2_{\tilde{y},-i}}.
$$
Finally, the leave-one-out predictive distribution can then be estimated as
$$
 p(y_i\,|\,y_{-i}) \approx \sum_{s=1}^S p(y_i\,|\,y_{-i}, \theta_{-i}^{(s)}),
$$
where $\theta_{-i}^{(s)}$ are draws from the posterior distribution
$p(\theta\,|\,y_{\mathrm{mis}(i)})$.

## Lagged SAR models

A common non-factorized multivariate normal model is the simultaneously
autoregressive (SAR) model, which is frequently used for spatially correlated
data. The lagged SAR model is defined as
$$
y = \rho Wy + \eta + \epsilon
$$
or equivalently
$$
(I - \rho W)y = \eta + \epsilon,
$$
where $\rho$ is the spatial correlation parameter and $W$ is a user-defined
weight matrix with $w_{ii} = 0$ as well as higher values $w_{ij}$ in the
off-diagonal for areas $i$ and $j$ closer to each other. In a linear model, the
predictor term $\eta$ is given by $\eta = X \beta$ with design matrix $X$ and
regression coefficients $\beta$. However, since the above equation holds for
arbitrary $\eta$, there is no need to restrict to linear models. If $\epsilon
\sim {\mathrm N}(0,\sigma^2 I)$, then
$$
(I - \rho W)y \sim {\mathrm N}(\eta, \sigma^2 I).
$$

This corresponds to the following log probability density function coded in
**Stan**:

```{r, eval = FALSE}
/* normal log-pdf for spatially lagged responses
 * Args:
 *   y: the response vector
 *   mu: mean parameter vector
 *   sigma: residual standard deviation
 *   rho: positive autoregressive parameter
 *   W: spatial weight matrix
 * Returns:  
 *   a scalar to be added to the log posterior
 */
real normal_lagsar_lpdf(vector y, vector mu, real sigma,
                        real rho, matrix W) {
  matrix[rows(y), rows(y)] W_new;
  vector[rows(y)] half_pred;
  real inv_sigma2;
  W_new = diag_matrix(rep_vector(1.0, rows(y))) - rho * W;
  half_pred  = W_new * (y - mdivide_left(W_new, mu));
  inv_sigma2 = 1 / sigma^2;
  return 0.5 * log_determinant(crossprod(W_new) * inv_sigma2) -
         0.5 * dot_self(half_pred) * inv_sigma2;
}
```

For the purpose of computing LOO-CV, it makes sense to rewrite the SAR model as
\begin{align}
y-(I-\rho W)^{-1}\eta &\sim {\mathrm N}(0, \sigma^2(I-\rho W)^{-1}(I-\rho W)^{-T}).
\end{align}
or with $\tilde{W}=(I-\rho W)$ more compactly
\begin{align}
y-\tilde{W}^{-1}\eta &\sim {\mathrm N}(0, \sigma^2(\tilde{W}^{T}\tilde{W})^{-1}).
\end{align}

Conditionally on $\rho$, $\eta$, and $\sigma$, this is has the same form as zero
mean Gaussian process. Accordingly, we can compute the leave-one-out predictive
densities with the equations by Sundararajan and Keerthi (2001) replacing $y$
with $(y-\tilde{W}^{-1}\eta)$ and using
$C = \sigma^2(\tilde{W}^{T}\tilde{W})^{-1}$.

## Case Study: Neighborhood Crime in Columbus

As an example, we will use a data set about the neighborhood crime in Columbus
initially described in Aneslin (1988) and shipped with the **spdep** package
in R.

```{r}
data(oldcol, package = "spdep")
```

Among others, the data contain the number of residential burglaries and vehicle
thefts per thousand households in the neighborhood (variable `CRIME`), the
housing value `HOVAL` as well as the household income `INC` (both in 1000 USD).
In addition, the data set includes the object `COL.nb`, from which the spatial
weight matrix of the neighborhoods can be constructed to account for spatial
dependency of the observations. For the analysis, we will be using the **brms**
package, LOO-CV will be performed by means of the **loo** package and plotting
with **ggplot2**.

```{r, cache=FALSE}
library(brms)
library(loo)
library(ggplot2)
theme_set(theme_default())
```

A model predicting `CRIME` with `INC` and `HOVAL`, while accounting for the
spatial dependency via an SAR structure, can be specified as follows.

```{r fit}
fit <- brm(
  CRIME ~ INC + HOVAL, data = COL.OLD,
  autocor = cor_lagsar(COL.nb),
  chains = 2, cores = 2
)
```

This fits the model in **Stan** using the log probability density function
provided above. From the summary output below we see the both higher income and
housing value predict *lower* crime rates in the neighborhood. Moreover, there
seems to be substantial spatial correlation between adjacent neighborhoods as
indicated by the `lagsar` parameter.

```{r}
summary(fit)
```

### Approximate LOO-CV

For the purpose of obtaining approximate LOO-CV, we need to compute the
pointwise log-likelihood values. For the above example, this can be done as
follows using the recipe explained in the former sections.

```{r}
params <- as.data.frame(fit)
y <- fit$data$CRIME
N <- length(y)
S <- nrow(params)
log_lik <- yloo <- sdloo <- matrix(nrow = S, ncol = N)
for (s in 1:S) {
  eta <- params$b_Intercept[s] +
    params$b_INC[s] * fit$data$INC +
    params$b_HOVAL[s] * fit$data$HOVAL
  IB <- diag(N) - params$lagsar[s] * fit$autocor$W
  Cinv <- t(IB) %*% IB / params$sigma[s]^2
  g <- Cinv %*% (y - solve(IB, eta))
  cbar <- diag(Cinv)
  yloo[s, ] <- y - g / cbar
  sdloo[s, ] <- sqrt(1 / cbar)
  log_lik[s, ] <- dnorm(y, yloo[s, ], sdloo[s, ], log = TRUE)
}
```

The goodness of the PSIS-LOO approximation can be investigated graphically by
plotting the Pareto-k estimates of each observations. Ideally, they should not
exceed $0.5$, but in practice the algorithm turns out to be robust up to values
of $0.7$ (Vehtari et al, 2017ab). In the plot below, we see that primarily the
fourth observation is problematic and may thus reduce the accuracy of the LOO-CV
approximation.

```{r, cache = FALSE}
log_ratios <- -log_lik
psis_result <- psis(log_ratios)
plot(psis_result, label_points = TRUE)
```

We can also check that the conditional leave-one-out predictive distribution
equations work correctly, for instance, using the last posterior draw:

```{r, cache = FALSE}
yloo_sub <- as.numeric(yloo[S, ])
sdloo_sub <- as.numeric(sdloo[S, ])
df <- data.frame(
  y = y, yloo = yloo_sub,
  ymin = yloo_sub - sdloo_sub * 2,
  ymax = yloo_sub + sdloo_sub * 2
)
ggplot(data=df, aes(x = y, y = yloo, ymin = ymin, ymax = ymax)) +
  geom_errorbar(width = 1, color = "skyblue3") +
  geom_point() +
  geom_abline(size = 1.2)
```

Finally, we compute the PSIS-LOO information criterion, which we will validate
using exact LOO-CV in the upcoming section.

```{r}
(psis_looic <- loo(log_lik))
```

### Exact LOO-CV

Exact LOO-CV for the above example is somewhat more involved, as we need to
allow missing value imputation and re-fit the model $N$ times. First, we create
a dummy model, which we can re-use afterwards.

```{r fit_dummy}
fit_dummy <- brm(
  CRIME | mi() ~ INC + HOVAL, data = COL.OLD,
  autocor = cor_lagsar(COL.nb), chains = 0
)
```

Next, we fit the model $N$ times each time leaving out a single observation and
compute the log predictive density for this observation. For obvious reasons,
this takes some time but is necessary in order to validate the approximate
LOO-CV results above.

```{r exact-loo-cv, results="hide", message=FALSE, warning=FALSE, cache = TRUE}
S <- 500
res <- vector("list", N)
log_lik <- matrix(NA, nrow = S, ncol = N)
for (i in seq_len(N)) {
  dat_mi <- COL.OLD
  dat_mi$CRIME[i] <- NA
  fit_i <- update(fit_dummy, newdata = dat_mi, chains = 1, iter = S * 2)
  params <- as.data.frame(fit_i)
  yloo <- sdloo <- rep(NA, S)
  for (s in seq_len(S)) {
    yt <- y
    yt[i] <- params$Ymi[s]
    eta <- params$b_Intercept[s] +
      params$b_INC[s] * fit_i$data$INC +
      params$b_HOVAL[s] * fit_i$data$HOVAL
    IB <- diag(N) - params$lagsar[s] * fit_i$autocor$W
    Cinv <- t(IB) %*% IB / params$sigma[s]^2
    g <- Cinv %*% (yt - solve(IB, eta))
    cbar <- diag(Cinv);
    yloo[s] <- yt[i] - g[i] / cbar[i]
    sdloo[s] <- sqrt(1 / cbar[i])
    log_lik[s, i] <- dnorm(y[i], yloo[s], sdloo[s], log = TRUE)
  }
  ypred <- rnorm(S, yloo, sdloo)
  res[[i]] <- data.frame(y = c(params$Ymi, ypred))
  res[[i]]$type <- rep(c("pp", "loo"), each = S)
  res[[i]]$obs <- i
}
res <- do.call(rbind, res)
```

A first step in the validation of the pointwise predictive density is to compare
the distribution of its implied response values for the left-out observation to
the distribution of the $y_{\mathrm{mis}}$ posterior-predictive values estimated
as part of the model. If the pointwise predictive density is correct, the two
distributions should match very closely (up to sampling error). In the plot
below, we overlay these two distributions for the first five observations and
see that they match very closely (as is the case for all $49$ observations of
the present example).

```{r yplots, fig.width=10, cache = FALSE}
res_sub <- res[res$obs %in% 1:5, ]
ggplot(res_sub, aes(y, fill = type)) +
  geom_density(alpha = 0.7) +
  facet_wrap("obs", scales = "free", ncol = 5)
```

In the final step, we compute expected log predictive density (elpd) based on
the exact LOO-CV and compare it to the approximate PSIS-LOO result computed
earlier.

```{r loo_exact, cache=FALSE}
exact_elpds <- apply(log_lik, 2, brms:::log_mean_exp)
round(exact_elpd <- sum(exact_elpds),1)
round(exact_looic <- -2 * exact_elpd,1)
```

The results of approximate and exact LOO-CV are similar but not as close as we
would expect **if** there were no problematic observations. We can investigate
this issue more closely by plotting the approximate against the exact pointwise
elpds.

```{r, fig.height=3}
df <- data.frame(
  approx_elpd = psis_looic$pointwise[, "elpd_loo"],
  exact_elpd = exact_elpds
)
ggplot(data=df, aes(x = approx_elpd, y = exact_elpd)) +
  geom_point() +
  geom_abline() +
  xlab("Approximate elpds") +
  ylab("Exact elpds") +
  xlim(c(-16, -3)) + ylim(c(-16, -3))
```

Summing over the pointwise elpds leaving out the problematic fourth observation
reveals practically equivalent results of approximate and exact LOO-CV:

````{r}
round(sum(psis_looic$pointwise[-4, "elpd_loo"]),1)
round(sum(exact_elpds[-4]),1)	   
```

Accordingly, the differences found when including all observations do not
indicate a bug in our implementation of the approximate LOO-CV but are rather
the result of violations of its assumptions.

## Working with Stan directly

So far, we have specified the models in brms and only used Stan implicitely
behind the scenes. This allowed us to focus on the primary purpose of validating
approximate LOO-CV for non-factorized models. However, we would also like to
show how everything can be set up in Stan directly. The Stan code brms generates
is human readable and so we can use it to learn some of the essential aspects of
Stan and the particular model we are implementing. The Stan code below was
extracted via `stancode(fit_dummy)`:

```{r, results="hide"}
"// generated with brms 2.2.0
functions {
  /* normal log-pdf for spatially lagged responses
   * Args:
   *   y: the response vector
   *   mu: mean parameter vector
   *   sigma: residual standard deviation
   *   rho: positive autoregressive parameter
   *   W: spatial weight matrix
   * Returns:  
   *   a scalar to be added to the log posterior
   */
   real normal_lagsar_lpdf(vector y, vector mu, real sigma,
                           real rho, matrix W) {
     matrix[rows(y), rows(y)] W_new;
     vector[rows(y)] half_pred;
     real inv_sigma2;
     W_new = diag_matrix(rep_vector(1.0, rows(y))) - rho * W;
     half_pred  = W_new * (y - mdivide_left(W_new, mu));
     inv_sigma2 = 1 / sigma^2;
     return 0.5 * log_determinant(crossprod(W_new) * inv_sigma2) -
            0.5 * dot_self(half_pred) * inv_sigma2;
   }
}
data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  int<lower=0> Nmi;  // number of missings
  int<lower=1> Jmi[Nmi];  // positions of missings
  int<lower=1> K;  // number of population-level effects
  matrix[N, K] X;  // population-level design matrix
  matrix[N, N] W;  // spatial weight matrix
  int prior_only;  // should the likelihood be ignored?
}
transformed data {
  int Kc = K - 1;
  matrix[N, K - 1] Xc;  // centered version of X
  vector[K - 1] means_X;  // column means of X before centering
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i] - means_X[i - 1];
  }
}
parameters {
  vector[Nmi] Ymi;  // estimated missings
  vector[Kc] b;  // population-level effects
  real temp_Intercept;  // temporary intercept
  real<lower=0> sigma;  // residual SD
  real<lower=0,upper=1> lagsar;  // SAR parameter
}
transformed parameters {
}
model {
  vector[N] Yl = Y;
  vector[N] mu = Xc * b + temp_Intercept;
  Yl[Jmi] = Ymi;
  // priors including all constants
  target += student_t_lpdf(temp_Intercept | 3, 34, 17);
  target += student_t_lpdf(sigma | 3, 0, 17)
    - 1 * student_t_lccdf(0 | 3, 0, 17);
  // likelihood including all constants
  if (!prior_only) {
    target += normal_lagsar_lpdf(Yl | mu, sigma, lagsar, W);
  }
}
generated quantities {
  // actual population-level intercept
  real b_Intercept = temp_Intercept - dot_product(means_X, b);
}"
```

Here, we want to focus on two aspects of the Stan code. First, the
log-likelihood is implemented by means of a user-defined function, which we
called `normal_lagsar_lpdf`. It implements the log-likelihood of the lag-SAR
model described above in a (hopefully) efficient and numerically stable way. The
`_lpdf` suffix is important to tell Stan that this is a log probability density
function. Second, the above Stan code nicely illustrates how to set up missing
value imputation. Instead of just computing the log-likelihood for the observed
responses `Y`, we define a new variable `Yl` which is equal to `Y` if the
reponse is observed and equal to `Ymi` if the response is missing. The latter is
in turn defined as a parameter and thus estimated along with all other paramters
of the model. More details about missing value imputation in Stan can be found
in Section 11 of the
[Stan manual](http://mc-stan.org/users/documentation/index.html).

Stan code extracted from brms is not only helpful when learning Stan, but can
also drastically speed up the specification of models in Stan which are not
support by brms. Suppose that brms can fit a model similar but not identical to
the desired model. Then, one can let brms generate the Stan code of the similar
model and customize it based on ones own needs. In this case, it is often
unpratical to use `stancode()` on a fitted model object, because we actually
don't need the fitted model. Instead, we can use `make_stancode()` to generate
the Stan code directly. The appropriate data can be prepared (and then manually
amended if needed) via `make_standata()`. Both Stan code and data can be passed
to (R)Stan via
`rstan::stan(model_code = <your Stan code>, data = <your Stan data>)`
in order to fit the model.

## Conclusion

In summary, we have shown how to set up and validate approximate and exact
LOO-CV for non-factorized multivariate normal models using brms and Stan. As an
example we considered spatial SAR models, but the presented recipe applies to
any model which can be expressed in terms of a multivariate normal likelihood.

<br />

## References

Anselin L. (1988). *Spatial econometrics: methods and models*. Dordrecht: Kluwer Academic.

Sundararajan S. & Keerthi S. S. (2001). Predictive approaches for choosing hyperparameters in Gaussian processes. *Neural Computation*, 13(5), 1103--1118.

Vehtari A., Mononen T., Tolvanen V., Sivula T., & Winther O. (2016). Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. *Journal of Machine Learning Research*, 17(103), 1--38. [Online](http://jmlr.org/papers/v17/14-540.html).

Vehtari A., Gelman A., & Gabry J. (2017a). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. *Statistics and Computing*, 27(5), 1413--1432. doi:10.1007/s11222-016-9696-4. [Online](http://link.springer.com/article/10.1007/s11222-016-9696-4). [arXiv preprint arXiv:1507.04544](https://arxiv.org/abs/1507.04544).

Vehtari A., Gelman A., & Gabry J. (2017b). Pareto smoothed importance sampling. [arXiv preprint arXiv:1507.02646](https://arxiv.org/abs/1507.02646).

<br />

## Appendix

### Appendix: Session information

```{r}
sessionInfo()
```

### Appendix: Licenses

* Code &copy; 2018, Aki Vehtari & Paul Buerkner, licensed under BSD-3.
* Text &copy; 2018, Aki Vehtari & Paul Buerkner, licensed under CC-BY-NC 4.0.
