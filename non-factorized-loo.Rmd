---
title: "Leave-one-out cross-validation for non-factorized models"
author: Aki Vehtari & Paul Buerkner
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache=TRUE, message=FALSE, error=FALSE, 
  warning=FALSE, comment=NA, out.width='95%'
)
```

## Introduction

For the computation of approximate leave-one-out cross-validation (LOO-CV) one needs to compute the *pointwise* log-likelihood of every response value $y_i$ where $i$ indicates observations. This is straight-forward for *factorized* models in which response values are conditionally independent given the parameters $\theta$:
$$
p(y | \theta) = \prod_{i=1}^n p(y_i | \theta).
$$
Here, as well as in the following, $p$ denotes probability density or mass functions depending on whether we are dealing with continuous or discrete random variables. In factorized models, the pointwise log-likelihood is simply given by $\log p(y_i | \theta)$.

The situation becomes more complicated when response values are not conditionally independent, and instead have residual dependency after taking the model parameters $\theta$ into account. The pointwise log-likelihood of such *non-factorized* models has the general form $\log p(y_i | y_{-i}, \theta)$, where $y_{-i}$ denotes all response values without the value of observation $i$.

## LOO-CV for multivariate normal models

Computing the pointwise log-likelihood for non-factorized models is not always possible but there is analytic solution available for a large class of multivariate normal models. These equations were initially derived by Sundararajan and Keerthi (2001) who focused on the special case of a normal zero-mean Gaussian process model with a prior covariance $K$ and residual standard deviation $\sigma$:
$$
y \sim N(0, K+\sigma^2 I),
$$
where $I$ is the identity matrix of appropriate dimension. Their results readily generalize to an arbitrary nondegenerate covariance matrix $C$ (ie it has to be invertible). The LOO predictive mean and standard deviation can be computed as follows:
\begin{align}
  \mu_{\tilde{y},-i} & = y_i-\bar{c}_{ii}^{-1}g_i \nonumber \\
  \sigma_{\tilde{y},-i} &= \sqrt{\bar{c}_{ii}^{-1}},
\end{align}
where 
\begin{align}
  g_i &= \left[C^{-1}y\right]_i \nonumber \\
  \bar{c}_{ii} &= \left[C^{-1}\right]_{ii}.
\end{align}

The log predictive density of observation $i$ is then computed as
$$
  \log p(y_i|y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi) 
  - \frac{1}{2}\log \sigma^2_{-i} 
  - \frac{1}{2}\frac{(y_i-\mu_{-i})^2}{\sigma^2_{-i}}.
$$
The above equation may also be written as follows:
$$
  \log p(y_i|y_{-i},\theta) 
  = - \frac{1}{2}\log(2\pi) 
  + \frac{1}{2}\log \bar{c}_{ii} 
  - \frac{1}{2}\frac{g_i^2}{\bar{c}_{ii}}
$$
(note that Vehtari et al. (2016) has a typo in the corresponding Equation 34). Together, this provides a recipe to obtain the pointwise log-likelihood for all models which can be expressed conditionally in terms of a multivariate normal model with an invertible covariance matrix $C$.

### Approximate LOO-CV using integrated importance-sampling

The above LOO equations for multivariate normal models are conditional
on parameters $\theta$. To obtain leave-one-out predictive density
$p(y_i|y_{-i})$ we need to integrate over $\theta$
$$
 p(y_i|y_{-i}) = \int p(y_i|y_{-i}, \theta) p(\theta|y_{-i}) d\theta,
$$
where $p(\theta|y_{-i})$ is the leave-one-out posterior for
$\theta$. To avoid sampling from $n$ leave-one-out posteriors, it is
possible to use posterior draws $\theta^{(s)}$ $(s=1,\ldots,S)$ from
the full posterior distribution $p(\theta|y)$ and approximate the
above integral with integrated importance sampling (Vehtari et al.,
2016, Section 3.6.1):
$$
 p(y_i|y_{-i}) \approx 
 \sum_{s=1}^S \frac{ p(y_i|y_{-i}, \theta^{(s)}) w_i^{(s)}}{w_i^{(s)}},
$$
where $w_i^{(s)}$ are importance weights. We first compute raw importance ratios
$$
  r_i^{(s)} \propto \frac{1}{p(y_i | y_{-i}, \theta^{(s)})},
$$
and then stabilize them using Pareto smoothed importance sampling
(PSIS, Vehtari et al, 2017b) method to obtain the weights
$w_i^{(s)}$. The resulting approximation is called Pareto-smoothed
importance-sampling leave-one-out (PSIS-LOO, Vehtari et al, 2017).

### Exact LOO-CV with re-fit

To validate the approximated LOO and to allow exact computation for a
small number of leave-one-out folds for which Pareto diagnostic
(Vehtari et al, 2017b) indicate unstable approximation (aka PSIS-LOO+,
Vehtari et al 2017a), we need to consider how to do exact
leave-one-out for non-factorizable model. In case of Gaussian process
which have the marginalization property, we could just drop from the
covariance $C$ the row and column corresponding to left out
observation. In general, this does not hold and to keep the original
prior we need to keep the full covariance matrix $C$ also when one of
the observations is left out. In this case we can model $y_i$ as
missing observation and estimate it along with all other model
parameters.

In case of conditional multivariate normal model, $\log
p(y_i|y_{-i})$ can be computed as follows. First, we model
$y_i$ as missing and call corresponding parameter
$y_{\mathrm{mis}}$. Then, we define
$$
y_{\mathrm{mis}(i)} = (y_1, \ldots, y_{i-1}, y_{\mathrm{mis}}, y_{i+1}, \ldots, y_n).
$$
Second, we compute the LOO predictive mean and standard deviations as above, but replace $y$ with $y_{\mathrm{mis}(i)}$ in the computation of $\mu_{\tilde{y},-i}$:
$$
\mu_{\tilde{y},-i} = y_{{\mathrm{mis}}(i)}-\bar{c}_{ii}^{-1}g_i,
$$
where 
$$
g_i = \left[ C^{-1} y_{\mathrm{mis}(i)} \right]_i.
$$
The conditional log predictive density is then computed with the above $\mu_{\tilde{y},-i}$ and the left out observation $y_i$
$$
  \log p(y_i|y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi) 
  - \frac{1}{2}\log \sigma^2_{\tilde{y},-i} 
  - \frac{1}{2}\frac{(y_i-\mu_{\tilde{y},-i})^2}{\sigma^2_{\tilde{y},-i}}.
$$
Leave-one-out predictive distribution is then easily estimated as
$$
 p(y_i|y_{-i}) \approx \sum_{s=1}^S p(y_i|y_{-i}, \theta_{-i}^{(s)}),
$$
where $\theta_{-i}^{(s)}$ are draws from
the posterior distribution $p(\theta|y_{\mathrm{mis}(i)})$.

## Lagged SAR models

A common non-factorized multivariate normal model is the simultaneously autoregressive (SAR) model, which is frequently used for spatially correlated data. The lagged SAR model is defined as
$$
y = \rho Wy + \eta + \epsilon
$$
or equivalently
$$
(I - \rho W)y = \eta + \epsilon,
$$
where $\rho$ is the spatial correlation parameter and $W$ is a user-defined weight matrix with $w_{ii} = 0$ as well as higher values $w_{ij}$ in the off-diagonal for areas $i$ and $j$ closer to each other. In a linear model, the predictor term $\eta$ is given by $\eta = X \beta$ with design matrix $X$ and regression coefficients $\beta$. However, since the above equation holds for arbitrary $\eta$, there is no need to restrict to linear models. If $\epsilon \sim N(0,\sigma^2 I)$, then
$$
(I - \rho W)y \sim {\mathrm N}(\eta, \sigma^2 I).
$$

This corresponds to the following log probability density function coded in **Stan**:

```{r, eval = FALSE}
/* normal log-pdf for spatially lagged responses 
 * Args: 
 *   y: the response vector 
 *   mu: mean parameter vector
 *   sigma: residual standard deviation
 *   rho: positive autoregressive parameter
 *   W: spatial weight matrix
 * Returns:  
 *   a scalar to be added to the log posterior 
 */ 
real normal_lagsar_lpdf(vector y, vector mu, real sigma, 
                        real rho, matrix W) { 
  matrix[rows(y), rows(y)] W_new;
  vector[rows(y)] half_pred;
  real inv_sigma2;
  W_new = diag_matrix(rep_vector(1.0, rows(y))) - rho * W;
  half_pred  = W_new * (y - mdivide_left(W_new, mu));
  inv_sigma2 = 1 / sigma^2;
  return 0.5 * log_determinant(crossprod(W_new) * inv_sigma2) - 
         0.5 * dot_self(half_pred) * inv_sigma2;
}
```

For the purpose of computing LOO-CV, it makes sense to rewrite the SAR model as
\begin{align}
y-(I-\rho W)^{-1}\eta &\sim {\mathrm N}(0, \sigma^2(I-\rho W)^{-1}(I-\rho W)^{-T}).
\end{align}
or with $\tilde{W}=(I-\rho W)$ more compactly
\begin{align}
y-\tilde{W}^{-1}\eta &\sim {\mathrm N}(0, \sigma^2(\tilde{W}^{T}\tilde{W})^{-1}).
\end{align}

Conditionally on $\rho$, $\eta$, and $\sigma$, this is has the same form as zero mean Gaussian process. Accordingly, we can compute the leave-one-out predictive densities with the equations by Sundararajan and Keerthi (2001) replacing $y$ with $(y-\tilde{W}^{-1}\eta)$ and using $C = \sigma^2(\tilde{W}^{T}\tilde{W})^{-1}$.

## Case Study: Neighborhood Crime in Columbus

As an example, we will use a data set about the neighborhood crime in Columbus initially described in Aneslin (1988) and shipped with the **spdep** package in R.

```{r}
data(oldcol, package = "spdep")
```

Among others, the data contain the number of residential burglaries and vehicle thefts per thousand households in the neighborhood (variable `CRIME`), the housing value `HOVAL` as well as the household income `INC` (both in 1000 USD). In addition, the data set includes the object `COL.nb`, from which the spatial weight matrix of the neighborhoods can be constructed to account for spatial dependency of the observations. For the analysis, we will be using the **brms** package, LOO-CV will be performed by means of the **loo** package and plotting with **ggplot2**.

```{r, cache=FALSE}
library(brms)
library(loo)
library(ggplot2)
theme_set(theme_default())
```

A model predicting `CRIME` with `INC` and `HOVAL`, while accounting for the spatial dependency via an SAR structure, can be specified as follows.

```{r fit}
fit <- brm(
  CRIME ~ INC + HOVAL, data = COL.OLD,
  autocor = cor_lagsar(COL.nb),
  chains = 2, cores = 2
)
```

This fits the model in **Stan** using the log probability density function provided above. From the summary output below we see the both higher income and housing value predict *lower* crime rates in the neighborhood. Moreover, there seems to be substantial spatial correlation between adjacent neighborhoods as indicated by the `lagsar` parameter.

```{r}
summary(fit)
```

### Approximate LOO-CV

For the purpose of obtaining approximate LOO-CV, we need to compute the pointwise log-likelihood values. For the above example, this can be done as follows using the recipe explained in the former sections.

```{r}
params <- as.data.frame(fit)
y <- fit$data$CRIME
N <- length(y)
S <- nrow(params)
log_lik <- yloo <- sdloo <- matrix(nrow = S, ncol = N)
for (s in 1:S) {
  eta <- params$b_Intercept[s] + 
    params$b_INC[s] * fit$data$INC + 
    params$b_HOVAL[s] * fit$data$HOVAL
  IB <- diag(N) - params$lagsar[s] * fit$autocor$W
  Cinv <- t(IB) %*% IB / params$sigma[s]^2
  g <- Cinv %*% (y - solve(IB, eta))
  cbar <- diag(Cinv)
  yloo[s, ] <- y - g / cbar
  sdloo[s, ] <- sqrt(1 / cbar)
  log_lik[s, ] <- dnorm(y, yloo[s, ], sdloo[s, ], log = TRUE)
}
```

The goodness of the PSIS-LOO approximation can be investigated graphically by plotting the Pareto-k estimates of each observations. Ideally, they should not exceed $0.5$, but in practice the algorithm turns out to be robust up to values of $0.7$ (Vehtari et al, 2017ab). In the plot below, we see that primarily the fourth observation is problematic and may thus reduce the accuracy of the LOO-CV approximation.

```{r, cache = FALSE}
log_ratios <- -log_lik
psis_result <- psis(log_ratios)
plot(psis_result, label_points = TRUE)
```

We can also check that the conditional leave-one-out predictive distribution equations work correctly, for instance, using the last posterior draw:

```{r, cache = FALSE}
yloo_sub <- as.numeric(yloo[S, ])
sdloo_sub <- as.numeric(sdloo[S, ])
df <- data.frame(
  y = y, yloo = yloo_sub, 
  ymin = yloo_sub - sdloo_sub * 2, 
  ymax = yloo_sub + sdloo_sub * 2
)
ggplot(data=df, aes(x = y, y = yloo, ymin = ymin, ymax = ymax)) +
  geom_errorbar(width = 1, color = "skyblue3") + 
  geom_point() +
  geom_abline(size = 1.2)
```

Finally, we compute the PSIS-LOO information criterion, which we will validate using exact LOO-CV in the upcoming section.

```{r}
(psis_looic <- loo(log_lik))
```

### Exact LOO-CV

Exact LOO-CV for the above example is somewhat more involved, as we need to allow missing value imputation and re-fit the model $N$ times. First, we create a dummy model, which we can re-use afterwards.

```{r fit_dummy}
fit_dummy <- brm(
  CRIME | mi() ~ INC + HOVAL, data = COL.OLD,
  autocor = cor_lagsar(COL.nb), chains = 0
)
```

Next, we fit the model $N$ times each time leaving out a single observation and compute the log predictive density for this observation. For obvious reasons, this takes some time but is necessary in order to validate the approximate LOO-CV results above.

```{r exact-loo-cv, results="hide", message=FALSE, warning=FALSE, cache = TRUE}
S <- 500
res <- vector("list", N)
log_lik <- matrix(NA, nrow = S, ncol = N)
for (i in seq_len(N)) {
  dat_mi <- COL.OLD
  dat_mi$CRIME[i] <- NA
  fit_i <- update(fit_dummy, newdata = dat_mi, chains = 1, iter = S * 2)
  params <- as.data.frame(fit_i)
  yloo <- sdloo <- rep(NA, S)
  for (s in seq_len(S)) {
    yt <- y
    yt[i] <- params$Ymi[s]
    eta <- params$b_Intercept[s] + 
      params$b_INC[s] * fit_i$data$INC + 
      params$b_HOVAL[s] * fit_i$data$HOVAL
    IB <- diag(N) - params$lagsar[s] * fit_i$autocor$W
    Cinv <- t(IB) %*% IB / params$sigma[s]^2
    g <- Cinv %*% (yt - solve(IB, eta))
    cbar <- diag(Cinv);
    yloo[s] <- yt[i] - g[i] / cbar[i]
    sdloo[s] <- sqrt(1 / cbar[i])
    log_lik[s, i] <- dnorm(y[i], yloo[s], sdloo[s], log = TRUE)
  }
  ypred <- rnorm(S, yloo, sdloo)
  res[[i]] <- data.frame(y = c(params$Ymi, ypred))
  res[[i]]$type <- rep(c("pp", "loo"), each = S)
  res[[i]]$obs <- i
}
res <- do.call(rbind, res)
```

A first step in the validation of the pointwise predictive density is to compare the distribution of its implied response values for the left-out observation to the distribution of the $y_{\mathrm{mis}}$ posterior-predictive values estimated as part of the model. If the pointwise predictive density is correct, the two distributions should match very closely (up to sampling error). In the plot below, we overlay these two distributions for the first five observations and see that they match very closely (as is the case for all $49$ observations of the present example).

```{r yplots, fig.width=10, cache = FALSE}
res_sub <- res[res$obs %in% 1:5, ]
ggplot(res_sub, aes(y, fill = type)) +
  geom_density(alpha = 0.7) + 
  facet_wrap("obs", scales = "free", ncol = 5)
```

In the final step, we compute expected log predictive density (elpd) based on the exact LOO-CV and compare it to the approximate PSIS-LOO result computed earlier.

```{r loo_exact, cache=FALSE}
exact_elpds <- apply(log_lik, 2, brms:::log_mean_exp)
round(exact_elpd <- sum(exact_elpds),1)
round(exact_looic <- -2 * exact_elpd,1)
```

The results of approximate and exact LOO-CV are similar but not as close as we would expect **if** there were no problematic observations. We can investigate this issue more closely by plotting the approximate against the exact pointwise elpds.

```{r, fig.height=3}
df <- data.frame(
  approx_elpd = psis_looic$pointwise[, "elpd_loo"],
  exact_elpd = exact_elpds
)
ggplot(data=df, aes(x = approx_elpd, y = exact_elpd)) +
  geom_point() + 
  geom_abline() +
  xlab("Approximate elpds") + 
  ylab("Exact elpds") +
  xlim(c(-16, -3)) + ylim(c(-16, -3))
```

Summing over the pointwise elpds leaving out the problematic fourth observation reveals practically equivalent results of approximate and exact LOO-CV:

````{r}
round(sum(psis_looic$pointwise[-4, "elpd_loo"]),1)
round(sum(exact_elpds[-4]),1)	   
```

Accordingly, the differences found when including all observations do not indicate a bug in our implementation of the approximate LOO-CV but are rather the result of violations of its assumptions.

In summary, we have shown how to set up and validate approximate and exact LOO-CV for non-factorized multivariate normal models. As an example we considered spatial SAR models, but the presented recipe applies to any model which can be expressed in terms of a multivariate normal likelihood.

<br />

## References

Anselin L. (1988). *Spatial econometrics: methods and models*. Dordrecht: Kluwer Academic.

Sundararajan S. & Keerthi S. S. (2001). Predictive approaches for choosing hyperparameters in Gaussian processes. *Neural Computation*, 13(5), 1103--1118.

Vehtari A., Mononen T., Tolvanen V., Sivula T., & Winther O. (2016). Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. *Journal of Machine Learning Research*, 17(103), 1--38. [Online](http://jmlr.org/papers/v17/14-540.html).

Vehtari A., Gelman A., & Gabry J. (2017a). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. *Statistics and Computing*, 27(5), 1413--1432. doi:10.1007/s11222-016-9696-4. [Online](http://link.springer.com/article/10.1007/s11222-016-9696-4). [arXiv preprint arXiv:1507.04544](https://arxiv.org/abs/1507.04544).

Vehtari A., Gelman A., & Gabry J. (2017b). Pareto smoothed importance sampling. [arXiv preprint arXiv:1507.02646](https://arxiv.org/abs/1507.02646).

<br />

## Appendix

### Appendix: Session information

```{r}
sessionInfo()
```

### Appendix: Licenses

* Code &copy; 2018, Aki Vehtari & Paul Buerkner, licensed under BSD-3.
* Text &copy; 2018, Aki Vehtari & Paul Buerkner, licensed under CC-BY-NC 4.0.
